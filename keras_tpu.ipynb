{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "keras_tpu.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chriswmann/keras-google-colab-tpu/blob/master/keras_tpu.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "eIdI3MhkNx9M",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Running Keras on Google Colab with TPU Acceleration\n",
        "\n",
        "## Introduction\n",
        "\n",
        "The purpose of this notebook is a demonstration of running Keras/TensorFlow on Google Colabatory with TPU acceleration.  There are a number of resources for this, such as Google's notebooks here:\n",
        "\n",
        "1. https://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/shakespeare_with_tpu_and_keras.ipynb\n",
        "2. https://colab.research.google.com/gist/ceshine/f196d6b030adb1ec3a8d0b50642709dc/keras-fashion-mnist-tpu.ipynb\n",
        "\n",
        "In order to try Keras with a TPU I needed a model.  I wanted to create something from scratch.  I happened to come across [this question](https://ai.stackexchange.com/questions/1508/which-neural-network-has-capabilities-of-sorting-input) on [ai.stackexchange.com](ai.stackexchange.com) and decided upon making a model which would sort a small array of random integers.\n",
        "\n",
        "Briefly, the main features this notebook comprises:\n",
        "1. A data generator which will produce train and test sets of random and sorted arrays respectively.\n",
        "2. A simple artificial neural network which can be run on CPU, GPU or TPU.\n",
        "3. A basic, home rolled random search for hyperparameter tuning.\n",
        "4. Simple visualisation of the results.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "hKRP9Gicl7vy",
        "colab_type": "code",
        "outputId": "056d5bfb-48ac-4ce8-bf7b-9fd431426d75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "import datetime as dt\n",
        "import keras.backend as K\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import pprint as pp\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow import keras\n",
        "\n",
        "def now():\n",
        "    return dt.datetime.now()\n",
        "\n",
        "try:\n",
        "    # This address identifies the TPU we'll use when configuring TensorFlow.\n",
        "    TPU_WORKER = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "    ACCELERATION = 'tpu'\n",
        "    print('TPU acceleration')\n",
        "except KeyError as e:\n",
        "    print('GPU acceleration')\n",
        "    ACCELERATION = 'gpu'\n",
        "        "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "TPU acceleration\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "OCi9vbNTIrku",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# define custom loss function to use RMSE in the NN model\n",
        "def rmse(y_true, y_pred):\n",
        "    \n",
        "        return K.sqrt(K.mean(K.square(y_pred - y_true), axis=-1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gIP36gfqpdFM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Data Generation"
      ]
    },
    {
      "metadata": {
        "id": "MD3I04uljIF1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class dataGenerator():\n",
        "    \n",
        "    \"\"\"Generate train and test data for machine learning exercises.\n",
        "    \n",
        "    This class generates train and test data in the form of random arrays \n",
        "    as inputs and sorted arrays as outputs.\n",
        "    \n",
        "    Usage:\n",
        "        Call set_config(size_to_sort, limit, train_samples, test_samples)\n",
        "        Call gen_data()\n",
        "        \n",
        "    Returns:\n",
        "        X_train, X_test, y_train, y_test\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        \n",
        "        \"\"\"Instantiate the class with a flag to indicate the data generation\n",
        "        configuration has not been set yet.\n",
        "        \"\"\"\n",
        "        \n",
        "        self.config = False\n",
        "        \n",
        "    def check_config_set(self):\n",
        "        \n",
        "        assert self.config, \"\"\"Error: Config not set. Call set_config(size_to_sort, limit, train_samples, test_samples)\"\"\"\n",
        "\n",
        "    def set_config(self, size_to_sort, limit, train_samples, test_samples):\n",
        "        \n",
        "        self.train_samples = train_samples    \n",
        "        self.test_samples = test_samples\n",
        "        self.size_to_sort = size_to_sort\n",
        "        self.limit = limit\n",
        "        self.config = True\n",
        "        \n",
        "    def gen_rand_array(self):\n",
        "        \n",
        "        return np.random.randint(0, self.limit, self.size_to_sort)\n",
        "\n",
        "    \n",
        "    def gen_data(self):\n",
        "        \n",
        "        self.check_config_set()\n",
        "        \n",
        "        self.X_train = np.array([self.gen_rand_array() for i in range(self.train_samples)]).astype(np.float64)\n",
        "        self.y_train = np.sort(self.X_train, axis=1)\n",
        "    \n",
        "        self.X_test = np.array([self.gen_rand_array() for i in range(self.test_samples)]).astype(np.float64)\n",
        "        self.y_test = np.sort(self.X_test, axis=1)\n",
        "        \n",
        "        return self.X_train, self.X_test, self.y_train, self.y_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WXIQRvCV_QPA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class checkUnique():\n",
        "    \n",
        "    \"\"\"Check that a set of hyperparameters have not been previously generated.\n",
        "    \n",
        "    As the random search function in this notebook is very basic, this class \n",
        "    can be used to avoid wasting compute time by storing hyperparameters\n",
        "    and checking if they've been assessed already.\n",
        "    \n",
        "    This class is used as part of a while loop in this notebook.\n",
        "    \n",
        "    Usage:\n",
        "        check_unique(activation, hidden_layers, units, optimiser)\n",
        "        \n",
        "    Returns:\n",
        "        True if the given hyperparameters have not been used already.\n",
        "        False if the given hyperparameters have already been used.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        \n",
        "        self.trial_param_history = []\n",
        "        \n",
        "    def check_unique(self, activation, hidden_layers, units, optimiser):\n",
        "        \n",
        "        self.activation = activation\n",
        "        self.hidden_layers = hidden_layers\n",
        "        self.units = units\n",
        "        self.optimiser = optimiser\n",
        "        self.trial_param_summary = (self.activation,\n",
        "                                    self.hidden_layers,\n",
        "                                    self.units,\n",
        "                                    self.optimiser)\n",
        "        \n",
        "        if self.trial_param_summary in self.trial_param_history:\n",
        "            return False\n",
        "        else:\n",
        "            self.trial_param_history.append(self.trial_param_summary)\n",
        "            return True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TNO6rU3ex7PJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class customLogger(keras.callbacks.Callback):\n",
        "    \n",
        "    \"\"\"Provide custom logging functionality for use during model fitting.\n",
        "    \n",
        "    Custom functionality includes the time taken for a given number (n) of \n",
        "    epochs to complete.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, n):\n",
        "        self.__n = n\n",
        "        self.__start_time = now()\n",
        "        self.__new_time = None\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        if epoch % self.__n == 0:\n",
        "            if self.__new_time:\n",
        "                self.__old_time = self.__new_time\n",
        "            else:\n",
        "                self.__old_time = self.__start_time\n",
        "            self.__new_time = now()\n",
        "            self.__curr_loss = logs.get('loss')\n",
        "            self.__val_loss = logs.get('val_loss')\n",
        "            self.__epoch_string = f'Epoch: {epoch:<5}'\n",
        "            self.__loss_string = f'loss: {self.__curr_loss:<15.6f}'\n",
        "            self.__val_loss_string = f'val loss: {self.__val_loss:<15.6f}'\n",
        "            self.__time_string = f'time: {now().strftime(\"%H:%M:%S\"):<10}'\n",
        "            self.__delta_t = str(self.__new_time - self.__old_time)\n",
        "            self.__delta_t_string = f'delta_t: {self.__delta_t:<20}'\n",
        "            print(self.__epoch_string,\n",
        "                  self.__loss_string,\n",
        "                  self.__val_loss_string,\n",
        "                  self.__time_string,\n",
        "                  self.__delta_t_string)\n",
        "            "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SLKgLue4xyl4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## learning rate reduction at plateua in loss\n",
        "patience = 10\n",
        "rlrop = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss',\n",
        "                                             factor=0.1,\n",
        "                                             patience=patience,\n",
        "                                             verbose=1,\n",
        "                                             mode='min',\n",
        "                                             min_delta=0.01,\n",
        "                                             cooldown=0,\n",
        "                                             min_lr=1e-6)\n",
        "\n",
        "## early stopping\n",
        "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
        "                                              min_delta=0.0001,\n",
        "                                              patience=patience * 2,\n",
        "                                              verbose=1,\n",
        "                                              mode='min')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xvY-fNXXnaoA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.utils import print_summary\n",
        "from tensorflow.keras.utils import Sequence\n",
        "\n",
        "adam = tf.train.AdamOptimizer()#lr=0.001, beta_1=0.9, beta_2=0.999)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "i8sAvw5lfUKn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class artificialNeuralNetwork():\n",
        "    \n",
        "    \"\"\"Build, train and assess an artificial neural network.\n",
        "    \n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, acceleration='tpu'):\n",
        "        \n",
        "        \"\"\"Instantiate the class with the acceleration type, the initial \n",
        "        trial count, a blank optimisation history and placeholders for\n",
        "        parameter values and predictions.\n",
        "        \"\"\"\n",
        "        \n",
        "        self.acceleration = acceleration\n",
        "        self.trial = 0\n",
        "        self.optimisation_history = {}\n",
        "        \n",
        "        self.params = None\n",
        "        self.y_pred = None\n",
        "        \n",
        "    def get_params(self, **kwargs):\n",
        "        \n",
        "        self.passed_args = kwargs\n",
        "        self.default_args = {'activation': 'relu',\n",
        "                             'callbacks': None,\n",
        "                             'epochs': 100,\n",
        "                             'mini_batch_size': None,\n",
        "                             'n_hidden_layers': 8,\n",
        "                             'n_trials': 20,\n",
        "                             'n_units': 4,\n",
        "                             'optimiser': 'adam',\n",
        "                             'size_to_sort': None,\n",
        "                             'validation_split': 0.25,\n",
        "                             'verbose': 0}\n",
        "        \n",
        "        self.diff = list(set(self.default_args.keys()) - set(self.passed_args.keys()))\n",
        "\n",
        "        assert not self.diff, f\"Error, missing required parameters and I don't know how to work around this yet. {self.diff}\"\n",
        "        \n",
        "        self.activation = self.passed_args['activation']\n",
        "        self.callbacks = self.passed_args['callbacks']\n",
        "        self.epochs = self.passed_args['epochs']\n",
        "\n",
        "        if self.passed_args['mini_batch_size']:\n",
        "            self.mini_batch_size = self.passed_args['mini_batch_size']\n",
        "        elif self.acceleration == 'gpu':\n",
        "            self.mini_batch_size = 64\n",
        "        elif self.acceleration == 'tpu':\n",
        "            self.mini_batch_size = 2096 * 8\n",
        "\n",
        "        self.n_hidden_layers = self.passed_args['n_hidden_layers']\n",
        "        self.n_units = self.passed_args['n_units']\n",
        "        self.n_trials = self.passed_args['n_trials']\n",
        "        self.optimiser = self.passed_args['optimiser']\n",
        "        self.size_to_sort = self.passed_args['size_to_sort']\n",
        "        self.validation_split = self.passed_args['validation_split']\n",
        "        self.verbose = self.passed_args['verbose']\n",
        "        \n",
        "        self.params = {'activation': self.activation,\n",
        "                       'callbacks': self.callbacks,\n",
        "                       'epochs': self.epochs,\n",
        "                       'mini_batch_size': self.mini_batch_size,\n",
        "                       'n_hidden_layers': self.n_hidden_layers,\n",
        "                       'n_units': self.n_units,\n",
        "                       'n_trials': self.n_trials,\n",
        "                       'optimiser': self.optimiser,\n",
        "                       'size_to_sort': self.size_to_sort,\n",
        "                       'validation_split': self.validation_split,\n",
        "                       'verbose': self.verbose}\n",
        "                   \n",
        "    def run_trial(self, X_train, y_train, X_test, y_test):\n",
        "        \n",
        "        \"\"\" Run a single trial comprising training and test.\n",
        "        \n",
        "        Arguments:\n",
        "        X_train -- train input data np.ndarray\n",
        "        y_train -- train target data np.ndarray\n",
        "        X_test -- test input data np.ndarray\n",
        "        y_test -- test target data np.ndarray\n",
        "        \"\"\"\n",
        "\n",
        "        self.X_train = X_train\n",
        "        self.y_train = y_train\n",
        "        self.X_test = X_test\n",
        "        self.y_test = y_test\n",
        "        \n",
        "        \n",
        "        self.dims_check = all([x.ndim==2 for x in [X_train, y_train, X_test, y_test]])\n",
        "        self.shapes_set = set([x.shape[1] for x in [X_train, y_train, X_test, y_test]])\n",
        "        self.shape_check = len(self.shapes_set) == 1\n",
        "        \n",
        "        assert self.dims_check, f\"Input arrays should be 2D, got {[x.ndim for x in [X_train, y_train, X_test, y_test]]}\"\n",
        "        assert self.shape_check, f\"Input arrays' second axes should match, got {[x.shape for x in [X_train, y_train, X_test, y_test]]}\"\n",
        "        assert self.params, \"No params defined, call get_params method.\"\n",
        "        \n",
        "        if not self.size_to_sort:\n",
        "            self.size_to_sort = list(self.shapes_set)[0]\n",
        "                   \n",
        "        print(\"==============================\")\n",
        "        print(f\"  Running trial {self.trial:<4} of {self.n_trials:>4}\")\n",
        "        print(\"==============================\")\n",
        "        \n",
        "        self.build_model()\n",
        "        self.fit(self.X_train, self.y_train),\n",
        "        self.predict(self.X_test)\n",
        "        self.error(self.y_pred, self.y_test)\n",
        "                \n",
        "        self.val_loss_scalar = self.history.history['loss'][-1]\n",
        "        self.optimisation_history[self.val_loss_scalar] = self.params\n",
        "        self.trial += 1\n",
        "\n",
        "    def build_model(self):\n",
        "        \n",
        "        \n",
        "        self.model = tf.keras.Sequential()\n",
        "\n",
        "        for i in range(self.n_hidden_layers):\n",
        "            self.model.add(tf.keras.layers.Dense(self.n_units))\n",
        "            self.model.add(tf.keras.layers.Activation(self.activation))\n",
        "            \n",
        "        self.input = tf.keras.Input(shape=(self.size_to_sort,), dtype=tf.float32)\n",
        "        self.model = self.model(self.input)\n",
        "        self.predictions = tf.keras.layers.Dense(self.size_to_sort)(self.model)\n",
        "\n",
        "        self.model = tf.keras.Model(inputs=self.input, outputs=self.predictions)\n",
        "        \n",
        "        if self.verbose:\n",
        "            print_summary(self.model)\n",
        "        else:\n",
        "            print(f\"Depth: {self.n_hidden_layers + 2:3}    Units: {self.n_units:3}\")\n",
        "        \n",
        "        if self.acceleration == 'tpu':\n",
        "            self.strategy = tf.contrib.tpu.TPUDistributionStrategy(\n",
        "                tf.contrib.cluster_resolver.TPUClusterResolver(TPU_WORKER))\n",
        "            \n",
        "            self.model = tf.contrib.tpu.keras_to_tpu_model(\n",
        "                self.model,\n",
        "                strategy=self.strategy)\n",
        "            \n",
        "        self.model.compile(optimizer=self.optimiser,\n",
        "                           loss=rmse)\n",
        "        \n",
        "        return self.model\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \n",
        "        self.X_fit = X\n",
        "        self.y_fit = y\n",
        "        \n",
        "        print(f\"Training for up to {self.epochs} epochs\")\n",
        "\n",
        "        self.history = self.model.fit(self.X_fit, self.y_fit,\n",
        "                                      validation_split=self.validation_split,\n",
        "                                      epochs=self.epochs,\n",
        "                                      batch_size=self.mini_batch_size,\n",
        "                                      verbose=self.verbose,\n",
        "                                      callbacks=self.callbacks)\n",
        "        return self.history\n",
        "    \n",
        "    def predict(self, X_pred):\n",
        "\n",
        "        self.X_pred = X_pred\n",
        "        \n",
        "        self.y_pred = self.model.predict(self.X_pred)\n",
        "    \n",
        "        return self.y_pred\n",
        "        \n",
        "    def error(self, y_pred, y_true):\n",
        "        \n",
        "        self.y_test = y_true\n",
        "        self.y_pred = y_pred\n",
        "            \n",
        "        self.val_loss = K.sqrt(K.mean(K.square(self.y_pred - self.y_test),\n",
        "                                      axis=-1))\n",
        "        \n",
        "    def get_results(self):\n",
        "        \n",
        "        return self.optimisation_history, self.history"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MyrjT2M1b9E-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from random import choice\n",
        "\n",
        "params = {'activations': ['relu', 'tanh'],\n",
        "          'n_hidden_layers': [2, 6, 14, 30, 62],\n",
        "          'n_units_list': [2, 4, 8, 16, 32],\n",
        "          'optimisers': ['adam', 'adagrad', 'rmsprop'],\n",
        "         }\n",
        "\n",
        "n_trials  = 20\n",
        "max_trials = np.prod([len(v) for k, v in params.items()])\n",
        "run_trials = min(n_trials, max_trials)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dLaZjxEU4_1R",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "epochs = 1000\n",
        "limit = 1_000_000\n",
        "mini_batch_size = 2048 * 8\n",
        "size_to_sort = 4\n",
        "test_samples = 8\n",
        "train_samples = 2048 * 8 * 5\n",
        "validation_split = 0.25\n",
        "verbose = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dwkMe6gx5Bk4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "dg = dataGenerator()\n",
        "dg.set_config(size_to_sort=size_to_sort, limit=limit,\n",
        "              train_samples=train_samples, test_samples=test_samples)\n",
        "\n",
        "X_train, X_test, y_train, y_test = dg.gen_data()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eQ4i3rPD5Czm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "mms = MinMaxScaler(feature_range=(-1, 1))\n",
        "X_train = mms.fit_transform(X_train)\n",
        "X_test = mms.fit_transform(X_test)\n",
        "\n",
        "callbacks = [customLogger(n=int(epochs/10)), rlrop, early_stop]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XKzSd7H45EUw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4148
        },
        "outputId": "6c97ce4c-d6dd-4065-a9e8-765488e4169f"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "objective = artificialNeuralNetwork(ACCELERATION)\n",
        "\n",
        "tf.keras.backend.clear_session()\n",
        "# INFO, WARN or ERRROR\n",
        "tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "\n",
        "cu = checkUnique()\n",
        "\n",
        "trial = 0\n",
        "while trial <= run_trials:\n",
        "    \n",
        "    activation = choice(params['activations'])\n",
        "    n_hidden_layers = choice(params['n_hidden_layers'])\n",
        "    n_units = choice(params['n_units_list'])\n",
        "    optimiser = choice(params['optimisers'])\n",
        "    \n",
        "    if cu.check_unique(activation, n_hidden_layers, n_units, optimiser):\n",
        "        \n",
        "        trial_params = {'activation': activation,\n",
        "                  'callbacks': callbacks,\n",
        "                  'epochs': epochs,\n",
        "                  'mini_batch_size': mini_batch_size,\n",
        "                  'n_hidden_layers': n_hidden_layers,\n",
        "                  'n_trials': n_trials,\n",
        "                  'n_units': n_units,\n",
        "                  'optimiser': optimiser,\n",
        "                  'size_to_sort': size_to_sort,\n",
        "                  'validation_split': validation_split,\n",
        "                  'verbose': verbose}\n",
        "        \n",
        "        objective.get_params(**trial_params)\n",
        "\n",
        "        objective.run_trial(X_train=X_train,\n",
        "                            y_train=y_train,\n",
        "                            X_test=X_test,\n",
        "                            y_test=y_test)\n",
        "\n",
        "        trial += 1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==============================\n",
            "  Running trial 0    of   20\n",
            "==============================\n",
            "Depth:  16    Units:   4\n",
            "Training for up to 1000 epochs\n",
            "Epoch: 0     loss: 561100.937500   val loss: 560761.975000   time: 05:51:34   delta_t: 0:00:32.581075      \n",
            "Epoch: 100   loss: 561097.812500   val loss: 560758.862500   time: 05:51:46   delta_t: 0:00:12.095617      \n",
            "Epoch: 200   loss: 561095.525000   val loss: 560756.612500   time: 05:51:58   delta_t: 0:00:11.848148      \n",
            "Epoch: 300   loss: 561093.375000   val loss: 560754.400000   time: 05:52:10   delta_t: 0:00:12.053251      \n",
            "Epoch: 400   loss: 561091.162500   val loss: 560752.150000   time: 05:52:22   delta_t: 0:00:11.794871      \n",
            "Epoch: 500   loss: 561088.925000   val loss: 560749.987500   time: 05:52:34   delta_t: 0:00:12.094143      \n",
            "Epoch: 600   loss: 561086.775000   val loss: 560747.800000   time: 05:52:46   delta_t: 0:00:12.079121      \n",
            "Epoch: 700   loss: 561084.525000   val loss: 560745.637500   time: 05:52:58   delta_t: 0:00:12.074880      \n",
            "Epoch: 800   loss: 561082.312500   val loss: 560743.300000   time: 05:53:11   delta_t: 0:00:12.186313      \n",
            "Epoch: 900   loss: 561080.075000   val loss: 560741.150000   time: 05:53:23   delta_t: 0:00:12.127369      \n",
            "==============================\n",
            "  Running trial 1    of   20\n",
            "==============================\n",
            "Depth:  16    Units:  32\n",
            "Training for up to 1000 epochs\n",
            "Epoch: 0     loss: 561100.962500   val loss: 560761.975000   time: 05:54:00   delta_t: 0:00:37.105964      \n",
            "Epoch: 100   loss: 115310.626562   val loss: 114753.889063   time: 05:54:12   delta_t: 0:00:12.424403      \n",
            "\n",
            "Epoch 00137: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "Epoch: 200   loss: 111020.842188   val loss: 111021.721875   time: 05:54:25   delta_t: 0:00:13.237980      \n",
            "\n",
            "Epoch 00233: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
            "Epoch: 300   loss: 110886.479688   val loss: 110878.457812   time: 05:54:38   delta_t: 0:00:12.411261      \n",
            "Epoch: 400   loss: 110848.150000   val loss: 110841.612500   time: 05:54:50   delta_t: 0:00:12.155437      \n",
            "Epoch: 500   loss: 110814.178125   val loss: 110809.504687   time: 05:55:02   delta_t: 0:00:12.445150      \n",
            "\n",
            "Epoch 00515: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
            "\n",
            "Epoch 00541: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
            "Epoch: 600   loss: 110808.390625   val loss: 110802.442187   time: 05:55:15   delta_t: 0:00:12.183381      \n",
            "Epoch 00617: early stopping\n",
            "==============================\n",
            "  Running trial 2    of   20\n",
            "==============================\n",
            "Depth:  16    Units:  16\n",
            "Training for up to 1000 epochs\n",
            "Epoch: 0     loss: 561100.812500   val loss: 560761.337500   time: 05:55:52   delta_t: 0:00:37.381255      \n",
            "Epoch: 100   loss: 561091.262500   val loss: 560752.150000   time: 05:56:04   delta_t: 0:00:11.857489      \n",
            "Epoch: 200   loss: 561083.700000   val loss: 560744.825000   time: 05:56:17   delta_t: 0:00:12.684927      \n",
            "Epoch: 300   loss: 561076.212500   val loss: 560737.200000   time: 05:56:29   delta_t: 0:00:11.947623      \n",
            "Epoch: 400   loss: 561068.712500   val loss: 560729.750000   time: 05:56:41   delta_t: 0:00:12.311451      \n",
            "Epoch: 500   loss: 561061.187500   val loss: 560722.187500   time: 05:56:53   delta_t: 0:00:12.440703      \n",
            "Epoch: 600   loss: 561053.675000   val loss: 560714.662500   time: 05:57:06   delta_t: 0:00:12.683383      \n",
            "Epoch: 700   loss: 561046.150000   val loss: 560707.200000   time: 05:57:19   delta_t: 0:00:12.628789      \n",
            "Epoch: 800   loss: 561038.612500   val loss: 560699.612500   time: 05:57:31   delta_t: 0:00:12.505210      \n",
            "Epoch: 900   loss: 561031.150000   val loss: 560692.062500   time: 05:57:43   delta_t: 0:00:12.194844      \n",
            "==============================\n",
            "  Running trial 3    of   20\n",
            "==============================\n",
            "Depth:  16    Units:   2\n",
            "Training for up to 1000 epochs\n",
            "Epoch: 0     loss: 561100.962500   val loss: 560761.987500   time: 05:58:41   delta_t: 0:00:57.856862      \n",
            "Epoch: 100   loss: 561098.787500   val loss: 560759.812500   time: 05:58:54   delta_t: 0:00:12.498948      \n",
            "Epoch: 200   loss: 561097.425000   val loss: 560758.437500   time: 05:59:07   delta_t: 0:00:12.885363      \n",
            "Epoch: 300   loss: 561096.112500   val loss: 560757.150000   time: 05:59:19   delta_t: 0:00:12.825359      \n",
            "Epoch: 400   loss: 561094.800000   val loss: 560755.762500   time: 05:59:32   delta_t: 0:00:12.927266      \n",
            "Epoch: 500   loss: 561093.500000   val loss: 560754.450000   time: 05:59:45   delta_t: 0:00:12.544655      \n",
            "Epoch: 600   loss: 561092.150000   val loss: 560753.100000   time: 05:59:57   delta_t: 0:00:12.640757      \n",
            "Epoch: 700   loss: 561090.812500   val loss: 560751.812500   time: 06:00:10   delta_t: 0:00:13.006218      \n",
            "Epoch: 800   loss: 561089.475000   val loss: 560750.462500   time: 06:00:23   delta_t: 0:00:12.818864      \n",
            "Epoch: 900   loss: 561088.150000   val loss: 560749.162500   time: 06:00:36   delta_t: 0:00:12.925776      \n",
            "==============================\n",
            "  Running trial 4    of   20\n",
            "==============================\n",
            "Depth:  32    Units:   8\n",
            "Training for up to 1000 epochs\n",
            "Epoch: 0     loss: 561100.737500   val loss: 560761.562500   time: 06:02:06   delta_t: 0:01:30.051037      \n",
            "Epoch: 100   loss: 561096.300000   val loss: 560757.337500   time: 06:02:21   delta_t: 0:00:14.621611      \n",
            "Epoch: 200   loss: 561094.787500   val loss: 560755.825000   time: 06:02:35   delta_t: 0:00:14.242701      \n",
            "Epoch: 300   loss: 561093.687500   val loss: 560754.750000   time: 06:02:50   delta_t: 0:00:14.622323      \n",
            "\n",
            "Epoch 00383: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
            "\n",
            "Epoch 00393: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
            "Epoch 00393: early stopping\n",
            "==============================\n",
            "  Running trial 5    of   20\n",
            "==============================\n",
            "Depth:  32    Units:  16\n",
            "Training for up to 1000 epochs\n",
            "Epoch: 0     loss: 561101.025000   val loss: 560762.050000   time: 06:05:09   delta_t: 0:02:18.964890      \n",
            "Epoch: 100   loss: 121933.914062   val loss: 120822.725000   time: 06:05:24   delta_t: 0:00:15.127276      \n",
            "\n",
            "Epoch 00131: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "Epoch: 200   loss: 113667.360937   val loss: 113633.937500   time: 06:05:44   delta_t: 0:00:20.431249      \n",
            "Epoch: 300   loss: 112857.014063   val loss: 112900.275000   time: 06:06:00   delta_t: 0:00:15.520671      \n",
            "Epoch: 400   loss: 112273.276563   val loss: 112199.223437   time: 06:06:15   delta_t: 0:00:15.326369      \n",
            "\n",
            "Epoch 00419: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
            "Epoch: 500   loss: 112027.484375   val loss: 112039.029688   time: 06:06:31   delta_t: 0:00:15.376016      \n",
            "\n",
            "Epoch 00508: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
            "\n",
            "Epoch 00522: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
            "Epoch 00532: early stopping\n",
            "==============================\n",
            "  Running trial 6    of   20\n",
            "==============================\n",
            "Depth:  32    Units:  16\n",
            "Training for up to 1000 epochs\n",
            "Epoch: 0     loss: 561100.925000   val loss: 560761.987500   time: 06:10:08   delta_t: 0:03:37.372531      \n",
            "Epoch: 100   loss: 111270.676562   val loss: 111248.457812   time: 06:10:25   delta_t: 0:00:17.000807      \n",
            "Epoch: 200   loss: 110733.451563   val loss: 110718.187500   time: 06:10:42   delta_t: 0:00:17.112063      \n",
            "Epoch: 300   loss: 110639.296875   val loss: 110623.257812   time: 06:10:59   delta_t: 0:00:17.148681      \n",
            "\n",
            "Epoch 00317: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "\n",
            "Epoch 00342: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
            "\n",
            "Epoch 00352: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
            "Epoch 00352: early stopping\n",
            "==============================\n",
            "  Running trial 7    of   20\n",
            "==============================\n",
            "Depth:  64    Units:   8\n",
            "Training for up to 1000 epochs\n",
            "Epoch: 0     loss: 561100.937500   val loss: 560761.975000   time: 06:16:33   delta_t: 0:05:33.433349      \n",
            "Epoch: 100   loss: 170798.956250   val loss: 170396.943750   time: 06:16:52   delta_t: 0:00:19.441077      \n",
            "Epoch: 200   loss: 167498.509375   val loss: 167129.659375   time: 06:17:11   delta_t: 0:00:19.253174      \n",
            "\n",
            "Epoch 00248: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
            "\n",
            "Epoch 00258: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
            "Epoch 00258: early stopping\n",
            "==============================\n",
            "  Running trial 8    of   20\n",
            "==============================\n",
            "Depth:  16    Units:   4\n",
            "Training for up to 1000 epochs\n",
            "Epoch: 0     loss: 561100.925000   val loss: 560761.975000   time: 06:23:42   delta_t: 0:06:30.926986      \n",
            "Epoch: 100   loss: 279788.306250   val loss: 278905.612500   time: 06:23:57   delta_t: 0:00:14.366469      \n",
            "Epoch: 200   loss: 231268.931250   val loss: 230491.515625   time: 06:24:11   delta_t: 0:00:14.430314      \n",
            "Epoch: 300   loss: 191629.109375   val loss: 190890.915625   time: 06:24:25   delta_t: 0:00:14.383596      \n",
            "Epoch: 400   loss: 155974.159375   val loss: 155256.128125   time: 06:24:40   delta_t: 0:00:14.421523      \n",
            "Epoch: 500   loss: 128411.025000   val loss: 128182.968750   time: 06:24:54   delta_t: 0:00:14.640384      \n",
            "Epoch: 600   loss: 117185.309375   val loss: 117162.132812   time: 06:25:10   delta_t: 0:00:15.049854      \n",
            "Epoch: 700   loss: 115370.190625   val loss: 115390.389063   time: 06:25:24   delta_t: 0:00:14.696372      \n",
            "Epoch: 800   loss: 114896.062500   val loss: 114935.885938   time: 06:25:39   delta_t: 0:00:14.718151      \n",
            "Epoch: 900   loss: 114550.317187   val loss: 114572.642187   time: 06:25:53   delta_t: 0:00:14.329503      \n",
            "\n",
            "Epoch 00990: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
            "==============================\n",
            "  Running trial 9    of   20\n",
            "==============================\n",
            "Depth:   8    Units:   4\n",
            "Training for up to 1000 epochs\n",
            "Epoch: 0     loss: 561101.025000   val loss: 560761.987500   time: 06:32:57   delta_t: 0:07:03.839213      \n",
            "Epoch: 100   loss: 560248.525000   val loss: 559893.837500   time: 06:33:11   delta_t: 0:00:13.947943      \n",
            "Epoch: 200   loss: 549674.850000   val loss: 549255.775000   time: 06:33:25   delta_t: 0:00:14.056528      \n",
            "Epoch: 300   loss: 511211.218750   val loss: 510687.143750   time: 06:33:39   delta_t: 0:00:14.029992      \n",
            "Epoch: 400   loss: 425272.237500   val loss: 424298.150000   time: 06:33:53   delta_t: 0:00:13.897553      \n",
            "Epoch: 500   loss: 292832.856250   val loss: 292113.775000   time: 06:34:07   delta_t: 0:00:14.177789      \n",
            "Epoch: 600   loss: 193175.871875   val loss: 193359.940625   time: 06:34:21   delta_t: 0:00:13.875712      \n",
            "Epoch: 700   loss: 159502.653125   val loss: 159752.975000   time: 06:34:35   delta_t: 0:00:14.047833      \n",
            "Epoch: 800   loss: 143378.581250   val loss: 143698.321875   time: 06:34:49   delta_t: 0:00:14.182236      \n",
            "Epoch: 900   loss: 134477.668750   val loss: 134681.153125   time: 06:35:03   delta_t: 0:00:14.143434      \n",
            "==============================\n",
            "  Running trial 10   of   20\n",
            "==============================\n",
            "Depth:  64    Units:  32\n",
            "Training for up to 1000 epochs\n",
            "Epoch: 0     loss: 561100.725000   val loss: 560761.562500   time: 06:47:05   delta_t: 0:12:01.978001      \n",
            "Epoch: 100   loss: 561082.837500   val loss: 560743.800000   time: 06:47:29   delta_t: 0:00:23.176407      \n",
            "Epoch: 200   loss: 561068.050000   val loss: 560729.012500   time: 06:47:51   delta_t: 0:00:22.602547      \n",
            "Epoch: 300   loss: 561053.275000   val loss: 560714.237500   time: 06:48:14   delta_t: 0:00:22.755526      \n",
            "Epoch: 400   loss: 561038.650000   val loss: 560699.537500   time: 06:48:37   delta_t: 0:00:23.153324      \n",
            "Epoch: 500   loss: 561024.037500   val loss: 560685.050000   time: 06:49:00   delta_t: 0:00:22.854817      \n",
            "Epoch: 600   loss: 561009.412500   val loss: 560670.412500   time: 06:49:23   delta_t: 0:00:22.773544      \n",
            "Epoch: 700   loss: 560994.850000   val loss: 560655.712500   time: 06:49:46   delta_t: 0:00:23.088977      \n",
            "Epoch: 800   loss: 560980.250000   val loss: 560641.250000   time: 06:50:09   delta_t: 0:00:23.162414      \n",
            "Epoch: 900   loss: 560965.700000   val loss: 560626.637500   time: 06:50:32   delta_t: 0:00:22.874698      \n",
            "==============================\n",
            "  Running trial 11   of   20\n",
            "==============================\n",
            "Depth:  64    Units:   2\n",
            "Training for up to 1000 epochs\n",
            "Epoch: 0     loss: 561100.950000   val loss: 560762.050000   time: 07:09:02   delta_t: 0:18:29.918892      \n",
            "Epoch: 100   loss: 561099.187500   val loss: 560760.150000   time: 07:09:26   delta_t: 0:00:24.045093      \n",
            "Epoch: 200   loss: 561097.862500   val loss: 560758.862500   time: 07:09:50   delta_t: 0:00:24.263035      \n",
            "Epoch: 300   loss: 561096.450000   val loss: 560757.562500   time: 07:10:14   delta_t: 0:00:24.308124      \n",
            "Epoch: 400   loss: 561095.137500   val loss: 560756.087500   time: 07:10:38   delta_t: 0:00:23.975761      \n",
            "Epoch: 500   loss: 561093.762500   val loss: 560754.850000   time: 07:11:03   delta_t: 0:00:24.307619      \n",
            "Epoch: 600   loss: 561092.462500   val loss: 560753.575000   time: 07:11:27   delta_t: 0:00:23.991282      \n",
            "Epoch: 700   loss: 561091.137500   val loss: 560752.087500   time: 07:11:51   delta_t: 0:00:24.598732      \n",
            "Epoch: 800   loss: 561089.787500   val loss: 560750.850000   time: 07:12:15   delta_t: 0:00:23.791708      \n",
            "Epoch: 900   loss: 561088.475000   val loss: 560749.525000   time: 07:12:39   delta_t: 0:00:24.179326      \n",
            "==============================\n",
            "  Running trial 12   of   20\n",
            "==============================\n",
            "Depth:  64    Units:   2\n",
            "Training for up to 1000 epochs\n",
            "Epoch: 0     loss: 561101.000000   val loss: 560761.975000   time: 07:35:17   delta_t: 0:22:38.173732      \n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
            "Epoch 00021: early stopping\n",
            "==============================\n",
            "  Running trial 13   of   20\n",
            "==============================\n",
            "Depth:   8    Units:  32\n",
            "Training for up to 1000 epochs\n",
            "Epoch: 0     loss: 561100.825000   val loss: 560761.675000   time: 08:01:50   delta_t: 0:26:32.683237      \n",
            "Epoch: 100   loss: 561082.862500   val loss: 560743.737500   time: 08:02:07   delta_t: 0:00:16.401900      \n",
            "Epoch: 200   loss: 561068.250000   val loss: 560729.150000   time: 08:02:23   delta_t: 0:00:16.590067      \n",
            "Epoch: 300   loss: 561053.575000   val loss: 560714.550000   time: 08:02:40   delta_t: 0:00:16.651641      \n",
            "Epoch: 400   loss: 561039.050000   val loss: 560699.900000   time: 08:02:56   delta_t: 0:00:16.404263      \n",
            "Epoch: 500   loss: 561024.500000   val loss: 560685.487500   time: 08:03:13   delta_t: 0:00:16.947365      \n",
            "Epoch: 600   loss: 561009.862500   val loss: 560670.837500   time: 08:03:30   delta_t: 0:00:16.466827      \n",
            "Epoch: 700   loss: 560995.262500   val loss: 560656.237500   time: 08:03:47   delta_t: 0:00:17.620325      \n",
            "Epoch: 800   loss: 560980.700000   val loss: 560641.687500   time: 08:04:04   delta_t: 0:00:16.735304      \n",
            "Epoch: 900   loss: 560966.125000   val loss: 560627.112500   time: 08:04:21   delta_t: 0:00:16.940791      \n",
            "==============================\n",
            "  Running trial 14   of   20\n",
            "==============================\n",
            "Depth:  64    Units:  32\n",
            "Training for up to 1000 epochs\n",
            "Epoch: 0     loss: 561100.950000   val loss: 560762.037500   time: 08:34:05   delta_t: 0:29:43.718671      \n",
            "\n",
            "Epoch 00057: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "Epoch: 100   loss: 114185.148438   val loss: 114475.365625   time: 08:36:11   delta_t: 0:02:06.367627      \n",
            "\n",
            "Epoch 00150: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
            "Epoch: 200   loss: 111611.004687   val loss: 111643.464063   time: 08:36:33   delta_t: 0:00:22.121260      \n",
            "Epoch: 300   loss: 111237.893750   val loss: 111264.206250   time: 08:36:55   delta_t: 0:00:22.225976      \n",
            "Epoch: 400   loss: 111031.196875   val loss: 111047.828125   time: 08:37:17   delta_t: 0:00:21.961041      \n",
            "\n",
            "Epoch 00451: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
            "\n",
            "Epoch 00462: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
            "Epoch: 500   loss: 110918.243750   val loss: 110942.651563   time: 08:37:40   delta_t: 0:00:22.637462      \n",
            "Epoch: 600   loss: 110896.059375   val loss: 110912.700000   time: 08:38:02   delta_t: 0:00:22.471677      \n",
            "Epoch 00610: early stopping\n",
            "==============================\n",
            "  Running trial 15   of   20\n",
            "==============================\n",
            "Depth:  16    Units:  16\n",
            "Training for up to 1000 epochs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nCZ06y6a5M21",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "optimisation_history, history = objective.get_results()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yPf0NWp7f6va",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = dg.gen_data()\n",
        "\n",
        "ann = artificialNeuralNetwork(ACCELERATION)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vpxGiYi65Te9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "best_params = optimisation_history[sorted(optimisation_history)[0]]\n",
        "\n",
        "ann.get_params(**best_params)\n",
        "\n",
        "ann.build_model()\n",
        "\n",
        "history = ann.fit(X_train, y_train)\n",
        "\n",
        "prediction = ann.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zO7ztrmrpr4S",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Plot training & validation loss values\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hYZtQcMUbPmb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "fig, axs = plt.subplots(test_samples, 4, sharey=True, figsize=(20, 60))\n",
        "x = np.arange(size_to_sort)\n",
        "for sample in range(test_samples):    \n",
        "    _ = sns.barplot(x, X_test[sample], ax=axs[sample][0])\n",
        "    _ = axs[sample][0].set_title(\"Input\")\n",
        "\n",
        "    _ = sns.barplot(x, y_test[sample], ax=axs[sample][1])\n",
        "    _ = axs[sample][1].set_title(\"Ground Truth\")\n",
        "\n",
        "    _ = sns.barplot(x, prediction[sample], ax=axs[sample][2])\n",
        "    _ = axs[sample][2].set_title(\"Prediction\")\n",
        "\n",
        "    _ = sns.barplot(x, prediction[sample] - y_test[sample], ax=axs[sample][3])\n",
        "    _ = axs[sample][3].set_title(\"Difference\")\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}