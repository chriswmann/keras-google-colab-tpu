{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "keras_tpu.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chriswmann/keras-google-colab-tpu/blob/master/keras_tpu.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "eIdI3MhkNx9M",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Running Keras on Google Colab with TPU Acceleration\n",
        "\n",
        "## Introduction\n",
        "\n",
        "There are a number of resources for running Keras on Google Collabatory with TPU acceleration.  For example, Google provides working notebooks such as these:\n",
        "\n",
        "1. https://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/shakespeare_with_tpu_and_keras.ipynb\n",
        "2. https://colab.research.google.com/gist/ceshine/f196d6b030adb1ec3a8d0b50642709dc/keras-fashion-mnist-tpu.ipynb\n",
        "\n",
        "In order to try Keras with a TPU I needed a model.  I wanted to create something from scratch and decided upon making a model which would sort a small array of random integers.\n",
        "\n",
        "Briefly, the main features this notebook comprises:\n",
        "1. A data generator which will produce train and test sets of random and sorted arrays respectively.\n",
        "2. A simple artificial neural network which can be run on CPU, GPU or TPU.\n",
        "3. A basic, home rolled random search for hyperparameter tuning.\n",
        "4. Simple visualisation of the results.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "hKRP9Gicl7vy",
        "colab_type": "code",
        "outputId": "dbe6d441-a7b0-4e09-f07c-2abba5ae158c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "import datetime as dt\n",
        "import keras.backend as K\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import pprint as pp\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow import keras\n",
        "\n",
        "def now():\n",
        "    return dt.datetime.now()\n",
        "\n",
        "try:\n",
        "    # This address identifies the TPU we'll use when configuring TensorFlow.\n",
        "    TPU_WORKER = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "    ACCELERATION = 'tpu'\n",
        "    print('TPU acceleration')\n",
        "except KeyError as e:\n",
        "    print('GPU acceleration')\n",
        "    ACCELERATION = 'gpu'\n",
        "        "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "TPU acceleration\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "OCi9vbNTIrku",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# define custom loss function to use RMSE in the NN model\n",
        "def rmse(y_true, y_pred):\n",
        "    \n",
        "        return K.sqrt(K.mean(K.square(y_pred - y_true), axis=-1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MD3I04uljIF1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class dataGenerator():\n",
        "    \n",
        "    def __init__(self):\n",
        "        \n",
        "        self.config = False\n",
        "        \n",
        "    def check_config_set(self):\n",
        "        \n",
        "        assert self.config, \"\"\"Error: Config not set. Call set_config(size_to_sort, limit, train_samples, test_samples)\"\"\"\n",
        "\n",
        "    def set_config(self, size_to_sort, limit, train_samples, test_samples):\n",
        "        \n",
        "        self.train_samples = train_samples    \n",
        "        self.test_samples = test_samples\n",
        "        self.size_to_sort = size_to_sort\n",
        "        self.limit = limit\n",
        "        self.config = True\n",
        "        \n",
        "    def gen_rand_array(self):\n",
        "        \n",
        "        return np.random.randint(0, self.limit, self.size_to_sort)\n",
        "\n",
        "    \n",
        "    def gen_data(self):\n",
        "        \n",
        "        self.check_config_set()\n",
        "        \n",
        "        self.X_train = np.array([self.gen_rand_array() for i in range(self.train_samples)]).astype(np.float64)\n",
        "        self.y_train = np.sort(self.X_train, axis=1)\n",
        "    \n",
        "        self.X_test = np.array([self.gen_rand_array() for i in range(self.test_samples)]).astype(np.float64)\n",
        "        self.y_test = np.sort(self.X_test, axis=1)\n",
        "        \n",
        "        return self.X_train, self.X_test, self.y_train, self.y_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WXIQRvCV_QPA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class checkUnique():\n",
        "    \n",
        "    def __init__(self):\n",
        "        \n",
        "        self.trial_param_history = []\n",
        "        \n",
        "    def check_unique(self, activation, hidden_layers, units, optimiser):\n",
        "        \n",
        "        self.activation = activation\n",
        "        self.hidden_layers = hidden_layers\n",
        "        self.units = units\n",
        "        self.optimiser = optimiser\n",
        "        self.trial_param_summary = (self.activation,\n",
        "                                    self.hidden_layers,\n",
        "                                    self.units,\n",
        "                                    self.optimiser)\n",
        "        \n",
        "        if self.trial_param_summary in self.trial_param_history:\n",
        "            return False\n",
        "        else:\n",
        "            self.trial_param_history.append(self.trial_param_summary)\n",
        "            return True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TNO6rU3ex7PJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class customLogger(keras.callbacks.Callback):\n",
        "    \n",
        "    def __init__(self, n):\n",
        "        self.__n = n\n",
        "        self.__start_time = now()\n",
        "        self.__new_time = None\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        if epoch % self.__n == 0:\n",
        "            if self.__new_time:\n",
        "                self.__old_time = self.__new_time\n",
        "            else:\n",
        "                self.__old_time = self.__start_time\n",
        "            self.__new_time = now()\n",
        "            self.__curr_loss = logs.get('loss')\n",
        "            self.__val_loss = logs.get('val_loss')\n",
        "            self.__epoch_string = f'Epoch: {epoch:<5}'\n",
        "            self.__loss_string = f'loss: {self.__curr_loss:<15.6f}'\n",
        "            self.__val_loss_string = f'val loss: {self.__val_loss:<15.6f}'\n",
        "            self.__time_string = f'time: {now().strftime(\"%H:%M:%S\"):<10}'\n",
        "            self.__delta_t = str(self.__new_time - self.__old_time)\n",
        "            self.__delta_t_string = f'delta_t: {self.__delta_t:<20}'\n",
        "            print(self.__epoch_string,\n",
        "                  self.__loss_string,\n",
        "                  self.__val_loss_string,\n",
        "                  self.__time_string,\n",
        "                  self.__delta_t_string)\n",
        "            "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SLKgLue4xyl4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## learning rate reduction at plateua in loss\n",
        "patience = 10\n",
        "rlrop = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss',\n",
        "                                             factor=0.1,\n",
        "                                             patience=patience,\n",
        "                                             verbose=1,\n",
        "                                             mode='min',\n",
        "                                             min_delta=0.01,\n",
        "                                             cooldown=0,\n",
        "                                             min_lr=1e-6)\n",
        "\n",
        "## early stopping\n",
        "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
        "                                              min_delta=0.0001,\n",
        "                                              patience=patience * 2,\n",
        "                                              verbose=1,\n",
        "                                              mode='min')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xvY-fNXXnaoA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.utils import print_summary\n",
        "from tensorflow.keras.utils import Sequence\n",
        "\n",
        "adam = tf.train.AdamOptimizer()#lr=0.001, beta_1=0.9, beta_2=0.999)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "i8sAvw5lfUKn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class objectiveFunction():\n",
        "    \n",
        "    def __init__(self, acceleration='tpu'):\n",
        "        \n",
        "        self.acceleration = acceleration\n",
        "        self.trial = 0\n",
        "        self.optimisation_history = {}\n",
        "        \n",
        "        self.params = None\n",
        "        self.y_pred = None\n",
        "        \n",
        "    def get_params(self, **kwargs):\n",
        "        \n",
        "        self.passed_args = kwargs\n",
        "        self.default_args = {'activation': 'relu',\n",
        "                             'callbacks': None,\n",
        "                             'epochs': 100,\n",
        "                             'mini_batch_size': None,\n",
        "                             'n_hidden_layers': 8,\n",
        "                             'n_trials': 20,\n",
        "                             'n_units': 4,\n",
        "                             'optimiser': 'adam',\n",
        "                             'size_to_sort': None,\n",
        "                             'validation_split': 0.25,\n",
        "                             'verbose': 0}\n",
        "        \n",
        "        self.diff = list(set(self.default_args.keys()) - set(self.passed_args.keys()))\n",
        "\n",
        "        assert not self.diff, f\"Error, missing required parameters and I don't know how to work around this yet. {self.diff}\"\n",
        "        \n",
        "        self.activation = self.passed_args['activation']\n",
        "        self.callbacks = self.passed_args['callbacks']\n",
        "        self.epochs = self.passed_args['epochs']\n",
        "\n",
        "        if self.passed_args['mini_batch_size']:\n",
        "            self.mini_batch_size = self.passed_args['mini_batch_size']\n",
        "        elif self.acceleration == 'gpu':\n",
        "            self.mini_batch_size = 64\n",
        "        elif self.acceleration == 'tpu':\n",
        "            self.mini_batch_size = 2096 * 8\n",
        "\n",
        "        self.n_hidden_layers = self.passed_args['n_hidden_layers']\n",
        "        self.n_units = self.passed_args['n_units']\n",
        "        self.n_trials = self.passed_args['n_trials']\n",
        "        self.optimiser = self.passed_args['optimiser']\n",
        "        self.size_to_sort = self.passed_args['size_to_sort']\n",
        "        self.validation_split = self.passed_args['validation_split']\n",
        "        self.verbose = self.passed_args['verbose']\n",
        "        \n",
        "        self.params = {'activation': self.activation,\n",
        "                       'callbacks': self.callbacks,\n",
        "                       'epochs': self.epochs,\n",
        "                       'mini_batch_size': self.mini_batch_size,\n",
        "                       'n_hidden_layers': self.n_hidden_layers,\n",
        "                       'n_units': self.n_units,\n",
        "                       'n_trials': self.n_trials,\n",
        "                       'optimiser': self.optimiser,\n",
        "                       'size_to_sort': self.size_to_sort,\n",
        "                       'validation_split': self.validation_split,\n",
        "                       'verbose': self.verbose}\n",
        "                   \n",
        "    def run_trial(self, X_train, y_train, X_test, y_test):\n",
        "        \n",
        "        \"\"\" Run a single trial comprising training and test.\n",
        "        \n",
        "        Arguments:\n",
        "        X_train -- train input data np.ndarray\n",
        "        y_train -- train target data np.ndarray\n",
        "        X_test -- test input data np.ndarray\n",
        "        y_test -- test target data np.ndarray\n",
        "        \"\"\"\n",
        "\n",
        "        self.X_train = X_train\n",
        "        self.y_train = y_train\n",
        "        self.X_test = X_test\n",
        "        self.y_test = y_test\n",
        "        \n",
        "        \n",
        "        self.dims_check = all([x.ndim==2 for x in [X_train, y_train, X_test, y_test]])\n",
        "        self.shapes_set = set([x.shape[1] for x in [X_train, y_train, X_test, y_test]])\n",
        "        self.shape_check = len(self.shapes_set) == 1\n",
        "        \n",
        "        assert self.dims_check, f\"Input arrays should be 2D, got {[x.ndim for x in [X_train, y_train, X_test, y_test]]}\"\n",
        "        assert self.shape_check, f\"Input arrays' second axes should match, got {[x.shape for x in [X_train, y_train, X_test, y_test]]}\"\n",
        "        assert self.params, \"No params defined, call get_params method.\"\n",
        "        \n",
        "        if not self.size_to_sort:\n",
        "            self.size_to_sort = list(self.shapes_set)[0]\n",
        "                   \n",
        "        print(\"==============================\")\n",
        "        print(f\"  Running trial {self.trial:<4} of {self.n_trials:>4}\")\n",
        "        print(\"==============================\")\n",
        "        \n",
        "        self.build_model()\n",
        "        self.fit(self.X_train, self.y_train),\n",
        "        self.predict(self.X_test)\n",
        "        self.error(self.y_pred, self.y_test)\n",
        "                \n",
        "        self.val_loss_scalar = self.history.history['loss'][-1]\n",
        "        self.optimisation_history[self.val_loss_scalar] = self.params\n",
        "        self.trial += 1\n",
        "\n",
        "    def build_model(self):\n",
        "        \n",
        "        \n",
        "        self.model = tf.keras.Sequential()\n",
        "\n",
        "        for i in range(self.n_hidden_layers):\n",
        "            self.model.add(tf.keras.layers.Dense(self.n_units))\n",
        "            self.model.add(tf.keras.layers.Activation(self.activation))\n",
        "            \n",
        "        self.input = tf.keras.Input(shape=(self.size_to_sort,), dtype=tf.float32)\n",
        "        self.model = self.model(self.input)\n",
        "        self.predictions = tf.keras.layers.Dense(self.size_to_sort)(self.model)\n",
        "\n",
        "        self.model = tf.keras.Model(inputs=self.input, outputs=self.predictions)\n",
        "        \n",
        "        if self.verbose:\n",
        "            print_summary(self.model)\n",
        "        else:\n",
        "            print(f\"Depth: {self.n_hidden_layers + 2:3}    Units: {self.n_units:3}\")\n",
        "        \n",
        "        if self.acceleration == 'tpu':\n",
        "            self.strategy = tf.contrib.tpu.TPUDistributionStrategy(\n",
        "                tf.contrib.cluster_resolver.TPUClusterResolver(TPU_WORKER))\n",
        "            \n",
        "            self.model = tf.contrib.tpu.keras_to_tpu_model(\n",
        "                self.model,\n",
        "                strategy=self.strategy)\n",
        "            \n",
        "        self.model.compile(optimizer=self.optimiser,\n",
        "                           loss=rmse)\n",
        "        \n",
        "        return self.model\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \n",
        "        self.X_fit = X\n",
        "        self.y_fit = y\n",
        "        \n",
        "        print(f\"Training for up to {self.epochs} epochs\")\n",
        "\n",
        "        self.history = self.model.fit(self.X_fit, self.y_fit,\n",
        "                                      validation_split=self.validation_split,\n",
        "                                      epochs=self.epochs,\n",
        "                                      batch_size=self.mini_batch_size,\n",
        "                                      verbose=self.verbose,\n",
        "                                      callbacks=self.callbacks)\n",
        "        return self.history\n",
        "    \n",
        "    def predict(self, X_pred):\n",
        "\n",
        "        self.X_pred = X_pred\n",
        "        \n",
        "        self.y_pred = self.model.predict(self.X_pred)\n",
        "    \n",
        "        return self.y_pred\n",
        "        \n",
        "    def error(self, y_pred, y_true):\n",
        "        \n",
        "        self.y_test = y_true\n",
        "        self.y_pred = y_pred\n",
        "            \n",
        "        self.val_loss = K.sqrt(K.mean(K.square(self.y_pred - self.y_test),\n",
        "                                      axis=-1))\n",
        "        \n",
        "    def get_results(self):\n",
        "        \n",
        "        return self.optimisation_history, self.history"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MyrjT2M1b9E-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from random import choice\n",
        "\n",
        "params = {'activations': ['relu', 'tanh'],\n",
        "          'n_hidden_layers': [2, 6, 14, 30, 62],\n",
        "          'n_units_list': [2, 4, 8, 16, 32],\n",
        "          'optimisers': ['adam', 'adagrad', 'rmsprop'],\n",
        "         }\n",
        "\n",
        "n_trials  = 20\n",
        "max_trials = np.prod([len(v) for k, v in params.items()])\n",
        "run_trials = min(n_trials, max_trials)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dLaZjxEU4_1R",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "epochs = 1000\n",
        "limit = 1_000_000\n",
        "mini_batch_size = 2048 * 8\n",
        "size_to_sort = 4\n",
        "test_samples = 8\n",
        "train_samples = 2048 * 8 * 5\n",
        "validation_split = 0.25\n",
        "verbose = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dwkMe6gx5Bk4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "dg = dataGenerator()\n",
        "dg.set_config(size_to_sort=size_to_sort, limit=limit,\n",
        "              train_samples=train_samples, test_samples=test_samples)\n",
        "\n",
        "X_train, X_test, y_train, y_test = dg.gen_data()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eQ4i3rPD5Czm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "mms = MinMaxScaler(feature_range=(-1, 1))\n",
        "X_train = mms.fit_transform(X_train)\n",
        "X_test = mms.fit_transform(X_test)\n",
        "\n",
        "callbacks = [customLogger(n=int(epochs/10)), rlrop, early_stop]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XKzSd7H45EUw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1190
        },
        "outputId": "9fc1c117-b4aa-4281-b5e9-d2f2eb2be78a"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "objective = objectiveFunction(ACCELERATION)\n",
        "\n",
        "tf.keras.backend.clear_session()\n",
        "# INFO, WARN or ERRROR\n",
        "tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "\n",
        "cu = checkUnique()\n",
        "\n",
        "trial = 0\n",
        "while trial <= run_trials:\n",
        "    \n",
        "    activation = choice(params['activations'])\n",
        "    n_hidden_layers = choice(params['n_hidden_layers'])\n",
        "    n_units = choice(params['n_units_list'])\n",
        "    optimiser = choice(params['optimisers'])\n",
        "    \n",
        "    if cu.check_unique(activation, n_hidden_layers, n_units, optimiser):\n",
        "        \n",
        "        trial_params = {'activation': activation,\n",
        "                  'callbacks': callbacks,\n",
        "                  'epochs': epochs,\n",
        "                  'mini_batch_size': mini_batch_size,\n",
        "                  'n_hidden_layers': n_hidden_layers,\n",
        "                  'n_trials': n_trials,\n",
        "                  'n_units': n_units,\n",
        "                  'optimiser': optimiser,\n",
        "                  'size_to_sort': size_to_sort,\n",
        "                  'validation_split': validation_split,\n",
        "                  'verbose': verbose}\n",
        "        \n",
        "        objective.get_params(**trial_params)\n",
        "\n",
        "        objective.run_trial(X_train=X_train,\n",
        "                            y_train=y_train,\n",
        "                            X_test=X_test,\n",
        "                            y_test=y_test)\n",
        "\n",
        "        trial += 1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==============================\n",
            "  Running trial 0    of   20\n",
            "==============================\n",
            "Depth:  32    Units:   2\n",
            "Training for up to 1000 epochs\n",
            "Epoch: 0     loss: 560508.712500   val loss: 561132.050000   time: 21:08:39   delta_t: 0:00:45.461574      \n",
            "Epoch: 100   loss: 560507.437500   val loss: 561130.737500   time: 21:08:52   delta_t: 0:00:12.829615      \n",
            "\n",
            "Epoch 00120: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
            "\n",
            "Epoch 00130: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
            "Epoch 00130: early stopping\n",
            "==============================\n",
            "  Running trial 1    of   20\n",
            "==============================\n",
            "Depth:  64    Units:   2\n",
            "Training for up to 1000 epochs\n",
            "Epoch: 0     loss: 560508.762500   val loss: 561132.000000   time: 21:10:21   delta_t: 0:01:28.949154      \n",
            "Epoch: 100   loss: 560507.150000   val loss: 561130.550000   time: 21:10:38   delta_t: 0:00:17.663682      \n",
            "Epoch: 200   loss: 560505.825000   val loss: 561129.137500   time: 21:10:56   delta_t: 0:00:17.811009      \n",
            "Epoch: 300   loss: 560504.487500   val loss: 561127.850000   time: 21:11:14   delta_t: 0:00:17.683323      \n",
            "Epoch: 400   loss: 560503.200000   val loss: 561126.500000   time: 21:11:31   delta_t: 0:00:17.355083      \n",
            "Epoch: 500   loss: 560501.825000   val loss: 561125.150000   time: 21:11:49   delta_t: 0:00:17.459313      \n",
            "Epoch: 600   loss: 560500.550000   val loss: 561123.875000   time: 21:12:06   delta_t: 0:00:17.264739      \n",
            "Epoch: 700   loss: 560499.250000   val loss: 561122.600000   time: 21:12:23   delta_t: 0:00:16.716409      \n",
            "Epoch: 800   loss: 560497.837500   val loss: 561121.212500   time: 21:12:40   delta_t: 0:00:16.945927      \n",
            "Epoch: 900   loss: 560496.562500   val loss: 561119.925000   time: 21:12:57   delta_t: 0:00:17.191557      \n",
            "==============================\n",
            "  Running trial 2    of   20\n",
            "==============================\n",
            "Depth:  16    Units:  32\n",
            "Training for up to 1000 epochs\n",
            "Epoch: 0     loss: 560508.750000   val loss: 561131.975000   time: 21:14:43   delta_t: 0:01:45.924041      \n",
            "Epoch: 100   loss: 116278.475000   val loss: 115864.037500   time: 21:14:55   delta_t: 0:00:12.657008      \n",
            "Epoch: 200   loss: 114559.164062   val loss: 114094.159375   time: 21:15:09   delta_t: 0:00:13.497447      \n",
            "\n",
            "Epoch 00209: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "\n",
            "Epoch 00228: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
            "Epoch: 300   loss: 110877.350000   val loss: 111038.579687   time: 21:15:26   delta_t: 0:00:17.190451      \n",
            "Epoch: 400   loss: 110833.473437   val loss: 110990.614062   time: 21:15:39   delta_t: 0:00:12.955906      \n",
            "Epoch: 500   loss: 110800.442187   val loss: 110959.690625   time: 21:15:53   delta_t: 0:00:13.662400      \n",
            "Epoch: 600   loss: 110769.482813   val loss: 110926.337500   time: 21:16:06   delta_t: 0:00:12.944968      \n",
            "Epoch: 700   loss: 110742.232813   val loss: 110901.260938   time: 21:16:18   delta_t: 0:00:12.722993      \n",
            "\n",
            "Epoch 00775: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
            "\n",
            "Epoch 00786: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
            "Epoch 00796: early stopping\n",
            "==============================\n",
            "  Running trial 3    of   20\n",
            "==============================\n",
            "Depth:   8    Units:   2\n",
            "Training for up to 1000 epochs\n",
            "Epoch: 0     loss: 560508.712500   val loss: 561132.000000   time: 21:18:02   delta_t: 0:01:44.133116      \n",
            "\n",
            "Epoch 00076: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
            "\n",
            "Epoch 00090: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
            "\n",
            "Epoch 00100: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
            "Epoch 00100: early stopping\n",
            "==============================\n",
            "  Running trial 4    of   20\n",
            "==============================\n",
            "Depth:   8    Units:   8\n",
            "Training for up to 1000 epochs\n",
            "Epoch: 0     loss: 560508.650000   val loss: 561131.725000   time: 21:20:02   delta_t: 0:02:00.051102      \n",
            "Epoch: 100   loss: 560503.562500   val loss: 561126.812500   time: 21:20:15   delta_t: 0:00:12.534267      \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nCZ06y6a5M21",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "optimisation_history, history = objective.get_results()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yPf0NWp7f6va",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = dg.gen_data()\n",
        "\n",
        "ann = objectiveFunction(ACCELERATION)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vpxGiYi65Te9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "best_params = optimisation_history[sorted(optimisation_history)[0]]\n",
        "\n",
        "ann.get_params(**best_params)\n",
        "\n",
        "ann.build_model()\n",
        "\n",
        "history = ann.fit(X_train, y_train)\n",
        "\n",
        "prediction = ann.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zO7ztrmrpr4S",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Plot training & validation loss values\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hYZtQcMUbPmb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "fig, axs = plt.subplots(test_samples, 4, sharey=True, figsize=(20, 60))\n",
        "x = np.arange(size_to_sort)\n",
        "for sample in range(test_samples):    \n",
        "    _ = sns.barplot(x, X_test[sample], ax=axs[sample][0])\n",
        "    _ = axs[sample][0].set_title(\"Input\")\n",
        "\n",
        "    _ = sns.barplot(x, y_test[sample], ax=axs[sample][1])\n",
        "    _ = axs[sample][1].set_title(\"Ground Truth\")\n",
        "\n",
        "    _ = sns.barplot(x, prediction[sample], ax=axs[sample][2])\n",
        "    _ = axs[sample][2].set_title(\"Prediction\")\n",
        "\n",
        "    _ = sns.barplot(x, prediction[sample] - y_test[sample], ax=axs[sample][3])\n",
        "    _ = axs[sample][3].set_title(\"Difference\")\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "e8VBcQp-dvuI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}